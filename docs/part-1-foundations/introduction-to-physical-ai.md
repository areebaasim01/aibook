---
sidebar_position: 1
title: "Chapter 1: Introduction to Physical AI"
description: Understanding the fundamentals of embodied artificial intelligence and its role in modern robotics
keywords: [physical ai, embodied intelligence, robotics, ai systems, embodied ai]
---

# Chapter 1: Introduction to Physical AI

> *"The next frontier of AI isn't just thinking‚Äîit's doing. Physical AI bridges the gap between digital intelligence and real-world action."*

---

<div className="learning-objectives">

## üéØ Learning Objectives

After completing this chapter, you will be able to:

- Define Physical AI and distinguish it from traditional software-based AI
- Explain the key components of embodied intelligence systems
- Describe the sensing-processing-acting loop in robotic systems
- Identify real-world applications of Physical AI
- Set up a development environment for Physical AI programming

</div>

---

## 1.1 What is Physical AI?

**Physical AI** (also called *Embodied AI*) refers to artificial intelligence systems that interact with the physical world through sensors and actuators. Unlike traditional AI that operates purely in the digital realm (like chatbots or recommendation systems), Physical AI must:

- **Sense** the real world through cameras, LiDAR, touch sensors, and more
- **Process** sensory input to understand the environment
- **Act** on the physical world through motors, grippers, and other actuators
- **Adapt** to unexpected situations in real-time

### The Physical AI Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTELLIGENCE LAYER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Planning   ‚îÇ ‚îÇ   Learning   ‚îÇ ‚îÇ  Decision Making    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    PROCESSING LAYER                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Perception ‚îÇ ‚îÇ  Localization‚îÇ ‚îÇ  State Estimation   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    INTERFACE LAYER                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ      Sensors        ‚îÇ    ‚îÇ        Actuators            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Cameras, LiDAR,     ‚îÇ    ‚îÇ    Motors, Grippers,        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ IMU, Touch, Audio   ‚îÇ    ‚îÇ    Speakers, Displays       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    PHYSICAL WORLD                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 1.2 The Sense-Process-Act Loop

At the heart of every Physical AI system is the **Sense-Process-Act** (SPA) loop. This continuous cycle enables robots to interact intelligently with their environment.

### The Loop in Action

```python
import time
from typing import Dict, Any
from dataclasses import dataclass

@dataclass
class SensorData:
    """Container for multi-modal sensor readings"""
    camera: Any = None
    lidar: Any = None
    imu: Dict[str, float] = None
    timestamp: float = 0.0

@dataclass
class RobotState:
    """Current state of the robot"""
    position: tuple = (0.0, 0.0, 0.0)
    orientation: tuple = (0.0, 0.0, 0.0, 1.0)  # quaternion
    velocity: tuple = (0.0, 0.0, 0.0)
    battery_level: float = 100.0

class PhysicalAIAgent:
    """
    Base class for a Physical AI agent demonstrating
    the Sense-Process-Act loop.
    """
    
    def __init__(self, name: str):
        self.name = name
        self.state = RobotState()
        self.is_running = False
    
    def sense(self) -> SensorData:
        """
        SENSE: Gather data from all sensors
        Returns a SensorData object with current readings
        """
        # In a real implementation, this would read from hardware
        return SensorData(
            camera=self._read_camera(),
            lidar=self._read_lidar(),
            imu=self._read_imu(),
            timestamp=time.time()
        )
    
    def process(self, sensor_data: SensorData) -> Dict[str, Any]:
        """
        PROCESS: Analyze sensor data and make decisions
        Returns a command dictionary
        """
        # Perception: Understand the environment
        objects = self._detect_objects(sensor_data.camera)
        obstacles = self._process_lidar(sensor_data.lidar)
        pose = self._estimate_pose(sensor_data.imu)
        
        # Planning: Decide what to do
        command = self._plan_action(objects, obstacles, pose)
        
        return command
    
    def act(self, command: Dict[str, Any]) -> bool:
        """
        ACT: Execute the planned action
        Returns success status
        """
        try:
            if command['type'] == 'move':
                self._move(command['velocity'], command['duration'])
            elif command['type'] == 'grasp':
                self._grasp(command['target'])
            elif command['type'] == 'speak':
                self._speak(command['message'])
            return True
        except Exception as e:
            print(f"Action failed: {e}")
            return False
    
    def run(self, frequency_hz: float = 10.0):
        """
        Main control loop running at specified frequency
        """
        self.is_running = True
        loop_period = 1.0 / frequency_hz
        
        print(f"ü§ñ {self.name} starting SPA loop at {frequency_hz}Hz")
        
        while self.is_running:
            loop_start = time.time()
            
            # The SPA Loop
            sensor_data = self.sense()      # 1. SENSE
            command = self.process(sensor_data)  # 2. PROCESS
            self.act(command)               # 3. ACT
            
            # Maintain loop frequency
            elapsed = time.time() - loop_start
            if elapsed < loop_period:
                time.sleep(loop_period - elapsed)
    
    def stop(self):
        """Gracefully stop the agent"""
        self.is_running = False
        print(f"üõë {self.name} stopped")
    
    # Private methods (stubs for demonstration)
    def _read_camera(self): return None
    def _read_lidar(self): return []
    def _read_imu(self): return {'ax': 0, 'ay': 0, 'az': 9.8}
    def _detect_objects(self, img): return []
    def _process_lidar(self, data): return []
    def _estimate_pose(self, imu): return self.state.position
    def _plan_action(self, obj, obs, pose): return {'type': 'idle'}
    def _move(self, vel, dur): pass
    def _grasp(self, target): pass
    def _speak(self, msg): print(f"üîä {msg}")


# Example usage
if __name__ == "__main__":
    agent = PhysicalAIAgent("PAI-001")
    
    # In practice, this would run continuously
    # agent.run(frequency_hz=10.0)
    
    # Demonstration of single loop iteration
    data = agent.sense()
    cmd = agent.process(data)
    agent.act({'type': 'speak', 'message': 'Hello, I am a Physical AI agent!'})
```

---

## 1.3 Key Characteristics of Physical AI

Physical AI systems differ from purely digital AI in several important ways:

### ‚ö° Real-Time Constraints

Physical systems operate in the real world where timing matters. A self-driving car can't take 5 seconds to decide whether to brake‚Äîdecisions must happen in milliseconds.

```python
import time
from functools import wraps

def timing_constraint(max_ms: float):
    """Decorator to enforce real-time constraints"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.perf_counter()
            result = func(*args, **kwargs)
            elapsed_ms = (time.perf_counter() - start) * 1000
            
            if elapsed_ms > max_ms:
                print(f"‚ö†Ô∏è Warning: {func.__name__} took {elapsed_ms:.2f}ms "
                      f"(limit: {max_ms}ms)")
            return result
        return wrapper
    return decorator

@timing_constraint(max_ms=50.0)
def process_sensor_data(data):
    """Must complete within 50ms for 20Hz control loop"""
    # Simulated processing
    time.sleep(0.03)  # 30ms - within budget
    return {"processed": True}
```

### üåç Uncertainty and Noise

The real world is messy. Sensors are imperfect, environments change, and unexpected events occur. Physical AI must handle uncertainty gracefully.

```python
import numpy as np
from typing import Tuple

class NoisySensor:
    """Simulates a real sensor with noise and occasional failures"""
    
    def __init__(self, noise_std: float = 0.1, failure_rate: float = 0.01):
        self.noise_std = noise_std
        self.failure_rate = failure_rate
    
    def read(self, true_value: float) -> Tuple[float, bool]:
        """
        Returns (noisy_reading, is_valid)
        """
        # Simulate sensor failure
        if np.random.random() < self.failure_rate:
            return (0.0, False)
        
        # Add Gaussian noise
        noisy_value = true_value + np.random.normal(0, self.noise_std)
        return (noisy_value, True)

# Example: Noisy distance sensor
distance_sensor = NoisySensor(noise_std=0.05, failure_rate=0.02)

# Take multiple readings and filter
true_distance = 1.5  # meters
readings = []

for _ in range(10):
    value, valid = distance_sensor.read(true_distance)
    if valid:
        readings.append(value)

estimated_distance = np.median(readings)  # Robust to outliers
print(f"True: {true_distance}m, Estimated: {estimated_distance:.3f}m")
```

### üîã Resource Constraints

Physical AI systems must manage limited resources: battery power, compute capacity, memory, and network bandwidth.

```python
from dataclasses import dataclass
from enum import Enum

class PowerMode(Enum):
    FULL = "full"
    BALANCED = "balanced"
    LOW_POWER = "low_power"
    CRITICAL = "critical"

@dataclass
class ResourceManager:
    """Manage robot resources and adapt behavior"""
    battery_percent: float = 100.0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    
    def get_power_mode(self) -> PowerMode:
        if self.battery_percent > 50:
            return PowerMode.FULL
        elif self.battery_percent > 25:
            return PowerMode.BALANCED
        elif self.battery_percent > 10:
            return PowerMode.LOW_POWER
        else:
            return PowerMode.CRITICAL
    
    def get_processing_budget(self) -> dict:
        """Return compute budget based on power mode"""
        budgets = {
            PowerMode.FULL: {
                'vision_fps': 30,
                'planning_hz': 10,
                'model_size': 'large'
            },
            PowerMode.BALANCED: {
                'vision_fps': 15,
                'planning_hz': 5,
                'model_size': 'medium'
            },
            PowerMode.LOW_POWER: {
                'vision_fps': 5,
                'planning_hz': 2,
                'model_size': 'small'
            },
            PowerMode.CRITICAL: {
                'vision_fps': 1,
                'planning_hz': 1,
                'model_size': 'tiny'
            }
        }
        return budgets[self.get_power_mode()]
```

---

## 1.4 Applications of Physical AI

Physical AI is transforming numerous industries:

### üè≠ Manufacturing & Logistics

- **Collaborative robots (cobots)** working alongside humans
- **Autonomous mobile robots (AMRs)** moving goods in warehouses
- **Quality inspection systems** detecting defects in real-time

### üè• Healthcare

- **Surgical robots** assisting with precision procedures
- **Rehabilitation exoskeletons** helping patients regain mobility
- **Care companion robots** providing assistance to elderly individuals

### üè† Home & Service

- **Vacuum robots** navigating complex home environments
- **Delivery robots** transporting packages autonomously
- **Humanoid assistants** helping with everyday tasks

### üöó Transportation

- **Autonomous vehicles** sensing and navigating roads
- **Drones** delivering goods and performing inspections
- **Last-mile delivery robots** navigating sidewalks and buildings

---

## 1.5 Hands-On: Your First Physical AI Simulation

Let's create a simple simulation of a robot navigating a 2D environment. This introduces key concepts we'll build upon throughout the course.

```python
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple
from dataclasses import dataclass

@dataclass
class Pose2D:
    x: float = 0.0
    y: float = 0.0
    theta: float = 0.0  # heading in radians

class SimpleRobot:
    """
    A simple 2D robot simulation demonstrating Physical AI concepts.
    """
    
    def __init__(self, start_pose: Pose2D = Pose2D()):
        self.pose = start_pose
        self.wheel_base = 0.5  # Distance between wheels (m)
        self.max_speed = 1.0   # Max velocity (m/s)
        self.sensor_range = 3.0  # LiDAR range (m)
        self.history: List[Tuple[float, float]] = [(start_pose.x, start_pose.y)]
    
    def sense_obstacles(self, obstacles: List[Tuple[float, float, float]]) -> List[float]:
        """
        Simulate LiDAR sensing - returns distances to obstacles
        obstacles: List of (x, y, radius) tuples
        """
        num_beams = 36  # 10-degree resolution
        distances = []
        
        for i in range(num_beams):
            angle = self.pose.theta + (i * 2 * np.pi / num_beams)
            min_dist = self.sensor_range
            
            # Check each obstacle
            for ox, oy, radius in obstacles:
                # Ray-circle intersection (simplified)
                dx = ox - self.pose.x
                dy = oy - self.pose.y
                dist_to_center = np.sqrt(dx**2 + dy**2)
                
                # Angle to obstacle
                obs_angle = np.arctan2(dy, dx)
                angle_diff = abs(self._wrap_angle(obs_angle - angle))
                
                # If beam points toward obstacle
                if angle_diff < 0.3 and dist_to_center - radius < min_dist:
                    min_dist = max(0, dist_to_center - radius)
            
            distances.append(min_dist)
        
        return distances
    
    def move(self, linear_vel: float, angular_vel: float, dt: float = 0.1):
        """
        Update robot pose using differential drive kinematics
        """
        # Clamp velocities
        linear_vel = np.clip(linear_vel, -self.max_speed, self.max_speed)
        
        # Update pose
        self.pose.x += linear_vel * np.cos(self.pose.theta) * dt
        self.pose.y += linear_vel * np.sin(self.pose.theta) * dt
        self.pose.theta = self._wrap_angle(self.pose.theta + angular_vel * dt)
        
        # Record history for visualization
        self.history.append((self.pose.x, self.pose.y))
    
    def navigate_to_goal(self, 
                         goal: Tuple[float, float], 
                         obstacles: List[Tuple[float, float, float]],
                         max_steps: int = 100) -> bool:
        """
        Simple navigation with obstacle avoidance
        """
        for step in range(max_steps):
            # Check if goal reached
            dist_to_goal = np.sqrt(
                (goal[0] - self.pose.x)**2 + 
                (goal[1] - self.pose.y)**2
            )
            if dist_to_goal < 0.2:
                print(f"‚úÖ Goal reached in {step} steps!")
                return True
            
            # Sense obstacles
            lidar = self.sense_obstacles(obstacles)
            
            # Simple reactive control
            front_clear = min(lidar[0:5] + lidar[-5:]) > 0.5
            left_clear = min(lidar[7:11]) > 0.5
            right_clear = min(lidar[25:29]) > 0.5
            
            # Calculate desired heading to goal
            desired_heading = np.arctan2(
                goal[1] - self.pose.y,
                goal[0] - self.pose.x
            )
            heading_error = self._wrap_angle(desired_heading - self.pose.theta)
            
            # Decide action
            if front_clear:
                linear_vel = 0.5
                angular_vel = 2.0 * heading_error  # Proportional control
            elif right_clear:
                linear_vel = 0.2
                angular_vel = -1.0  # Turn right
            elif left_clear:
                linear_vel = 0.2
                angular_vel = 1.0  # Turn left
            else:
                linear_vel = -0.2  # Back up
                angular_vel = 0.0
            
            # Execute motion
            self.move(linear_vel, angular_vel)
        
        print("‚ùå Failed to reach goal in time")
        return False
    
    def _wrap_angle(self, angle: float) -> float:
        """Wrap angle to [-pi, pi]"""
        while angle > np.pi:
            angle -= 2 * np.pi
        while angle < -np.pi:
            angle += 2 * np.pi
        return angle
    
    def visualize(self, 
                  obstacles: List[Tuple[float, float, float]],
                  goal: Tuple[float, float]):
        """Create a visualization of the robot's journey"""
        fig, ax = plt.subplots(1, 1, figsize=(10, 10))
        
        # Plot obstacles
        for ox, oy, radius in obstacles:
            circle = plt.Circle((ox, oy), radius, color='red', alpha=0.5)
            ax.add_patch(circle)
        
        # Plot path
        path_x = [p[0] for p in self.history]
        path_y = [p[1] for p in self.history]
        ax.plot(path_x, path_y, 'b-', linewidth=2, label='Robot Path')
        
        # Plot start and end
        ax.plot(path_x[0], path_y[0], 'go', markersize=15, label='Start')
        ax.plot(goal[0], goal[1], 'g*', markersize=20, label='Goal')
        
        # Plot robot current position
        ax.plot(self.pose.x, self.pose.y, 'bs', markersize=12, label='Robot')
        
        ax.set_xlim(-1, 10)
        ax.set_ylim(-1, 10)
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        ax.legend()
        ax.set_title('Simple Robot Navigation')
        
        plt.tight_layout()
        return fig


# Run simulation
if __name__ == "__main__":
    # Define environment
    obstacles = [
        (3.0, 3.0, 0.8),   # (x, y, radius)
        (5.0, 2.0, 0.5),
        (4.0, 5.0, 0.6),
        (7.0, 4.0, 0.7),
    ]
    goal = (8.0, 8.0)
    
    # Create and run robot
    robot = SimpleRobot(Pose2D(x=1.0, y=1.0, theta=0.0))
    success = robot.navigate_to_goal(goal, obstacles, max_steps=200)
    
    # Visualize
    fig = robot.visualize(obstacles, goal)
    plt.savefig('robot_navigation.png', dpi=150)
    plt.show()
    
    print(f"\nFinal position: ({robot.pose.x:.2f}, {robot.pose.y:.2f})")
```

---

## 1.6 Summary

In this chapter, we introduced the foundational concepts of Physical AI:

| Concept | Key Takeaway |
|---------|--------------|
| **Physical AI** | AI that interacts with the real world through sensors and actuators |
| **SPA Loop** | The Sense-Process-Act cycle is the heartbeat of robotic systems |
| **Real-Time** | Physical systems must make decisions within strict time constraints |
| **Uncertainty** | The real world is noisy; systems must be robust to sensor errors |
| **Resources** | Robots must manage limited compute, power, and memory |

---

## üìù Exercises

### Exercise 1.1: Extend the SPA Loop

Modify the `PhysicalAIAgent` class to include error recovery. If an action fails, the robot should:
1. Log the failure
2. Return to a safe state
3. Attempt an alternative action

### Exercise 1.2: Noise Filtering

Implement a **Kalman Filter** or **moving average filter** to smooth the noisy sensor readings. Compare the results with the raw median filter.

### Exercise 1.3: Navigation Challenge

Extend the `SimpleRobot` simulation:
- Add more obstacles
- Implement a more sophisticated path planning algorithm (e.g., A*)
- Add dynamic obstacles that move over time

---

## üìö Further Reading

- **Robotics, Vision and Control** by Peter Corke
- **Probabilistic Robotics** by Thrun, Burgard, and Fox
- **Modern Robotics** by Lynch and Park

---

<div style={{textAlign: 'center', marginTop: '2rem'}}>

[‚Üê Back to Introduction](/docs/intro) | [Next: Humanoid Robotics Fundamentals ‚Üí](/docs/part-1-foundations/humanoid-robotics-fundamentals)

</div>
